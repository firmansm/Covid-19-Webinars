# -*- coding: utf-8 -*-
"""webinar2012-2021.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1O1PKl2lDHbwn_1ktMm_QHkSMDxXFqZsv
"""

#!pip install plotly==5.5.0

"""### Mount Google Drive"""

from google.colab import drive
drive.mount('/content/drive')

import numpy as np
import pandas as pd

"""### Load Dataset"""

df = pd.read_csv('insert_file_location', low_memory=False)

len(df)

df.head()

"""### Crostabs"""

# Extract year from post date
df['year'] = pd.to_datetime(df['postdate']).dt.year
pd.crosstab(df.country,df.year)

#Unique Page per year per country
pd.crosstab(
    df.country, df.year, margins=True, 
    values=df.pagename, aggfunc=pd.Series.nunique
)

# Label posts containing "free", "gratis", and "percuma"
pd.crosstab(df.country,df['message'].str.contains('free|gratis|percuma'))

# Label posts containing "free", "gratis", and "percuma"
pd.crosstab(df.country,df['linktext'].str.contains('free|gratis|percuma'))

# Label posts containing "free", "gratis", and "percuma"
pd.crosstab(df.country,df['desc'].str.contains('free|gratis|percuma'))

# Label posts containing "free", "gratis", and "percuma"
df['free'] = 0 

# find keywords in messages
df.loc[df['message'].str.contains('free|gratis|percuma') | 
       df['linktext'].str.contains('free|gratis|percuma') |
       df['desc'].str.contains('free|gratis|percuma'),
       'free'] = 1 # then set class to 1
pd.crosstab(df.country,df.free)

# Label posts not containing "free", "gratis", and "percuma"
df['notfree'] = 1 

# find keywords in messages
df.loc[df['message'].str.contains('free|gratis|percuma') | 
       df['linktext'].str.contains('free|gratis|percuma') |
       df['desc'].str.contains('free|gratis|percuma'),
       'notfree'] = 0 
pd.crosstab(df.country,df.notfree)

# Grouped by Year & Country
pd.crosstab(df.country,[df.year, df.free])

# Grouped by Year & Country, Normalized by Number of Page
df.groupby(['year','country']).apply(lambda x: np.average(len(x.free), weights=len(pd.unique(x.pagename)))).unstack(level=0)

"""### Create New Aggregated Dataframe"""

params = {    
  'pagename': pd.Series.nunique,
  'username' : 'count',
  'free': 'sum',
  'notfree' : 'sum'
}
dd = df.groupby(['country','year']).agg(params).reset_index()
dd.tail()

#average free webinar per page per year
dd['avefree'] = dd.free/dd.pagename
dd['avenotfree'] = dd.notfree/dd.pagename
dd.head()

"""## Visualizations"""

import plotly.express as px

"""## Number of unique pages and total posts per country"""

fig = px.line(dd, 
              x='year', 
              y=['pagename','username'],
              facet_col_wrap=6, 
              facet_col="country", 
              width= 1200,
              height = 550,
              title="Number of Verified University Pages and Their Webinar Posts on Facebook Between 2012 and 2021", 
              category_orders={"country":["AU", "GB", "US", "IN", "ID", "MY"]},
              labels={"year": ""})

fig.add_vrect(x0="2012", x1="2019", annotation_text="covid-19", annotation_position="right", fillcolor="grey", opacity=0.15, line_width=0)
fig.update_traces(line=dict(width=5))
fig.for_each_annotation(lambda a: a.update(text=a.text.split("=")[-1]))
newlegends = {"pagename": "Verified Pages", "username": "Webinar Posts"}
fig.for_each_trace(lambda t: t.update(name = newlegends[t.name],
                                      legendgroup = newlegends[t.name],
                                      hovertemplate = t.hovertemplate.replace(t.name, newlegends[t.name])
                                     ))
fig.update_layout(title_font_size=22, legend_title="",yaxis_title="Count")
fig.update_xaxes(tickangle=-45)
fig.show()

fig = px.line(dd, 
              x='year', 
              y=['free','notfree'],
              facet_col_wrap=6, 
              facet_col="country", 
              width= 1200,
              height = 600,
              title="Number of Free vs Not Free Webinar Posts by Verified University Pages Between 2012 and 2021", 
              category_orders={"country":["AU", "GB", "US", "IN", "ID", "MY"]},
              labels={"year": ""})

fig.add_vrect(x0="2012", x1="2019", annotation_text="covid-19", annotation_position="right", fillcolor="grey", opacity=0.15, line_width=0)
fig.update_traces(line=dict(width=5))
fig.for_each_annotation(lambda a: a.update(text=a.text.split("=")[-1]))
newlegends = {"free": "Free Webinars", "notfree": "Not Free Webinar"}
fig.for_each_trace(lambda t: t.update(name = newlegends[t.name],
                                      legendgroup = newlegends[t.name],
                                      hovertemplate = t.hovertemplate.replace(t.name, newlegends[t.name])
                                     ))
fig.update_layout(title_font_size=22, legend_title="",yaxis_title="Count")
fig.update_xaxes(tickangle=-45)
fig.show()

fig = px.line(dd, 
              x='year', 
              y=['avefree','avenotfree'],
              facet_col_wrap=6, 
              facet_col="country", 
              width= 1200,
              height = 550,
              title="Average Number of Free vs Not Free Webinar Posts by Verified University Pages Between 2012 and 2021", 
              category_orders={"country":["AU", "GB", "US", "IN", "ID", "MY"]},
              labels={"year": ""},
              color_discrete_map={
                 "avenotfree": "orange",
                 "avefree": "purple"
              })

fig.add_vrect(x0="2012", x1="2019", annotation_text="covid-19", annotation_position="right", fillcolor="grey", opacity=0.15, line_width=0)
fig.update_traces(line=dict(width=5))
fig.for_each_annotation(lambda a: a.update(text=a.text.split("=")[-1]))
newlegends = {"avefree": "Free Webinars", "avenotfree": "Not Free Webinar"}
fig.for_each_trace(lambda t: t.update(name = newlegends[t.name],
                                      legendgroup = newlegends[t.name],
                                      hovertemplate = t.hovertemplate.replace(t.name, newlegends[t.name])
                                     ))
fig.update_layout(title_font_size=22, legend_title="",yaxis_title="Count")
fig.update_xaxes(tickangle=-45)
fig.show()

"""## Statistics"""

from scipy.stats import chi2_contingency

dc = df 

# Create new column covid19
dc['covid19'] = 1

# find keywords in messages
dc.loc[dc['year'] < 2020,'covid19'] = 0 
dc.head()

params = {    
  'pagename': 'count',
}
dc.groupby(['country','covid19']).agg(params).reset_index()

# Create new dataframe for AU
dcAU = dc[dc['country'].str.contains('AU')]
dcAUct = pd.crosstab(dcAU['covid19'], df['free'], normalize='index')
dcAUct

# Chi-square test of independence.
c, p, dof, expected = chi2_contingency(dcAUct)
c, p

# Create new dataframe for UK
dcUK = dc[dc['country'].str.contains('GB')]
dcUKct = pd.crosstab(dcUK['covid19'], df['free'], normalize='index')
dcUKct

# Chi-square test of independence.
c, p, dof, expected = chi2_contingency(dcUKct)
c, p

# Create new dataframe for US
dcUS = dc[dc['country'].str.contains('US')]
dcUSct = pd.crosstab(dcUS['covid19'], df['free'], normalize='index')
dcUSct

# Chi-square test of independence.
c, p, dof, expected = chi2_contingency(dcUSct)
c, p

# Create new dataframe for IN
dcIN = dc[dc['country'].str.contains('IN')]
dcINct = pd.crosstab(dcIN['covid19'], df['free'], normalize='index')
dcINct

# Chi-square test of independence.
c, p, dof, expected = chi2_contingency(dcINct)
c, p

# Create new dataframe for ID
dcID = dc[dc['country'].str.contains('ID')]
dcIDct = pd.crosstab(dcID['covid19'], df['free'], normalize='index')
dcIDct

# Chi-square test of independence.
c, p, dof, expected = chi2_contingency(dcIDct)
c, p

# Create new dataframe for MY
dcMY = dc[dc['country'].str.contains('MY')]
dcMYct = pd.crosstab(dcID['covid19'], df['free'], normalize='index')
dcMYct

# Chi-square test of independence.
c, p, dof, expected = chi2_contingency(dcMYct)
c, p

"""## 2018-2021 only"""

# Create new dataframe for 2018-2021
dc1 = df[df['year'] >= 2018]
dc1

# Create new column precovid19
dc1['covid19'] = 1

# find keywords in messages
dc1.loc[dc1['year'] <  2020,'covid19'] = 0 
dc1



"""### Australia"""

# Create new dataframe for AU
dc1AU = dc1[dc1['country'].str.contains('AU')]
dc1AUct = pd.crosstab(dc1AU['covid19'], dc1AU['free'], normalize='index')
dc1AUct

# Chi-square test of independence.
c, p, dof, expected = chi2_contingency(dc1AUct)
c, p

"""### United Kingdom"""

# Create new dataframe for UK
dc1UK = dc1[dc1['country'].str.contains('GB')]
dc1UKct = pd.crosstab(dc1UK['covid19'], dc1UK['free'], normalize='index')
dc1UKct

# Chi-square test of independence.
c, p, dof, expected = chi2_contingency(dc1UKct)
c, p

"""### United States"""

# Create new dataframe for US
dc1US = dc1[dc1['country'].str.contains('US')]
dc1USct = pd.crosstab(dc1US['covid19'], dc1US['free'], normalize='index')
dc1USct

# Chi-square test of independence.
c, p, dof, expected = chi2_contingency(dc1USct)
c, p

"""### India"""

# Create new dataframe for IN
dc1IN = dc1[dc1['country'].str.contains('IN')]
dc1INct = pd.crosstab(dc1IN['covid19'], dc1IN['free'], normalize='index')
dc1INct

# Chi-square test of independence.
c, p, dof, expected = chi2_contingency(dc1INct)
c, p

"""### Indonesia"""

# Create new dataframe for ID
dc1ID = dc1[dc1['country'].str.contains('ID')]
dc1IDct = pd.crosstab(dc1ID['covid19'], dc1ID['free'], normalize='index')
dc1IDct

# Chi-square test of independence.
c, p, dof, expected = chi2_contingency(dc1IDct)
c, p

"""### Malaysia"""

# Create new dataframe for MY
dc1MY = dc1[dc1['country'].str.contains('MY')]
dc1MYct = pd.crosstab(dc1MY['covid19'], dc1MY['free'], normalize='index')
dc1MYct

# Chi-square test of independence.
c, p, dof, expected = chi2_contingency(dc1MYct)
c, p